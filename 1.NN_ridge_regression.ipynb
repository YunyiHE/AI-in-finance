{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=num_features, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# reshape 1d arrays to 2d arrays\n",
    "trainy = trainy.reshape(len(trainy), 1)\n",
    "testy = testy.reshape(len(trainy), 1)\n",
    "\n",
    "# create scaler\n",
    "scaler = StandardScaler()\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(trainy)\n",
    "# transform training dataset\n",
    "trainy = scaler.transform(trainy)\n",
    "# transform test dataset\n",
    "testy = scaler.transform(testy)\n",
    "\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(trainX)\n",
    "# transform training dataset\n",
    "trainX = scaler.transform(trainX)\n",
    "# transform test dataset\n",
    "testX = scaler.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 234us/step - loss: 1.1247 - val_loss: 0.2733\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.2984 - val_loss: 0.1389\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.1306 - val_loss: 0.1317\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1101 - val_loss: 0.1117\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1051 - val_loss: 0.1073\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1042 - val_loss: 0.1069\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1037 - val_loss: 0.1071\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1034 - val_loss: 0.1077\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1068\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1070\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1068\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 33us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1033 - val_loss: 0.1070\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1037 - val_loss: 0.1072\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1037 - val_loss: 0.1072\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1038 - val_loss: 0.1073\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1067\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1077\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1032 - val_loss: 0.1067\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1033 - val_loss: 0.1071\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1035 - val_loss: 0.1068\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1073\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1071\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1037 - val_loss: 0.1069\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1069\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1074\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1037 - val_loss: 0.1073\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1071\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1033 - val_loss: 0.1073\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1068\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1072\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1075\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1036 - val_loss: 0.1066\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1037 - val_loss: 0.1079\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1070\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1069\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1033 - val_loss: 0.1069\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1069\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.106 - 0s 32us/step - loss: 0.1035 - val_loss: 0.1073\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1074\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1039 - val_loss: 0.1069\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1033 - val_loss: 0.1074\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1065\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1071\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1071\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1071\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1064\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1075\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1035 - val_loss: 0.1073\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1037 - val_loss: 0.1066\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1068\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1033 - val_loss: 0.1071\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1035 - val_loss: 0.1073\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1035 - val_loss: 0.1067\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1075\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1067\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1034 - val_loss: 0.1072\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1070\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1070\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1072\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1071\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1033 - val_loss: 0.1071\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1068\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1070\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1034 - val_loss: 0.1073\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.1034 - val_loss: 0.1066\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1075\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1068\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1033 - val_loss: 0.1073\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1070\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1070\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 0s 28us/step - loss: 0.1036 - val_loss: 0.1069\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1037 - val_loss: 0.1073\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1035 - val_loss: 0.1067\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1034 - val_loss: 0.1073\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1033 - val_loss: 0.1071\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1070\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1032 - val_loss: 0.1070\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1035 - val_loss: 0.1071\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1071\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 28us/step - loss: 0.1033 - val_loss: 0.1072\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1037 - val_loss: 0.1074\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1071\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1071\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1036 - val_loss: 0.1073\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.1037 - val_loss: 0.1068\n",
      "500/500 [==============================] - 0s 10us/step\n",
      "500/500 [==============================] - 0s 12us/step\n",
      "Train loss: 0.103, Test loss: 0.107\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mlp with scaled inputs outputs on the regression problem\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=20, activation='linear', kernel_initializer='he_uniform', kernel_regularizer= keras.regularizers.l2(l=.1)))\n",
    "#compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=1)\n",
    "# evaluate the model\n",
    "train_e = model.evaluate(trainX, trainy, verbose=1)\n",
    "test_e = model.evaluate(testX, testy, verbose=1)\n",
    "print('Train loss: %.3f, Test loss: %.3f' % (train_e, test_e)) \n",
    "#plot loss during training\n",
    "plt.title('Loss / Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you change the ridge regression to a lasso regression? You change the regularization from l2 to l1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
