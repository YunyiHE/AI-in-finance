{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=4000, n_features=num_features, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 2000\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# reshape 1d arrays to 2d arrays\n",
    "trainy = trainy.reshape(len(trainy), 1)\n",
    "testy = testy.reshape(len(trainy), 1)\n",
    "\n",
    "# create scaler\n",
    "scaler = StandardScaler()\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(trainy)\n",
    "# transform training dataset\n",
    "trainy = scaler.transform(trainy)\n",
    "# transform test dataset\n",
    "testy = scaler.transform(testy)\n",
    "\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(trainX)\n",
    "# transform training dataset\n",
    "trainX = scaler.transform(trainX)\n",
    "# transform test dataset\n",
    "testX = scaler.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(trainy, columns=['y'])\n",
    "df['lab']=np.where(df.y.shift(-1)>df.y,1,0) #like price prediction\n",
    "trainy=df.lab.fillna(0).values\n",
    "df = pd.DataFrame(testy, columns=['y'])\n",
    "df['lab']=np.where(df.y.shift(-1)>df.y,1,0) #like price prediction\n",
    "testy=df.lab.fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 0.7468 - acc: 0.4730\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5258 - acc: 0.2225\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5158 - acc: 0.1710\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5101 - acc: 0.2450\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5065 - acc: 0.3085\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5046 - acc: 0.3575\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5035 - acc: 0.3770\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5029 - acc: 0.3810\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5025 - acc: 0.4115\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5022 - acc: 0.4295\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5020 - acc: 0.4530\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5018 - acc: 0.4555\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5017 - acc: 0.4605\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5015 - acc: 0.4655\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5014 - acc: 0.4720\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5013 - acc: 0.4805\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5012 - acc: 0.4855\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5012 - acc: 0.4855\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5011 - acc: 0.4925\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5011 - acc: 0.4930\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5010 - acc: 0.4960\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5010 - acc: 0.4955\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5010 - acc: 0.4965\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5009 - acc: 0.4965\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5009 - acc: 0.4980\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5009 - acc: 0.4980\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5008 - acc: 0.4995\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5008 - acc: 0.4990\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5008 - acc: 0.4990\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5008 - acc: 0.4995\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5008 - acc: 0.4990\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5008 - acc: 0.4995\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5007 - acc: 0.4990\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5007 - acc: 0.4995\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.3438 - acc: 0.656 - 0s 26us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5006 - acc: 0.4995\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5005 - acc: 0.4995\n",
      "Best: 0.499493 using {'optimizer': 'RMSprop'}\n",
      "0.322015 (0.024161) with: {'optimizer': 'SGD'}\n",
      "0.499493 (0.009693) with: {'optimizer': 'RMSprop'}\n",
      "0.452990 (0.017799) with: {'optimizer': 'Adagrad'}\n",
      "0.498993 (0.010367) with: {'optimizer': 'Adadelta'}\n",
      "0.497493 (0.010793) with: {'optimizer': 'Adam'}\n",
      "0.483489 (0.016322) with: {'optimizer': 'Adamax'}\n",
      "0.492489 (0.015850) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier with scaled inputs outputs on the regression problem\n",
    "\n",
    "def create_model(optimizer='adam'):\n",
    "    # define model (see links at the bottom)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=20, units=64, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='linear', kernel_regularizer=keras.regularizers.l2(l=0.01)))\n",
    "    #compile the model\n",
    "    model.compile(loss='hinge', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# define the grid search hyper parameter options and dictionary\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "#create the grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(trainX, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine the model using the best optimizer from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.6969 - acc: 0.4260 - val_loss: 0.5474 - val_acc: 0.2645\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5262 - acc: 0.1700 - val_loss: 0.5267 - val_acc: 0.1135\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5165 - acc: 0.1425 - val_loss: 0.5196 - val_acc: 0.1505\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5106 - acc: 0.2005 - val_loss: 0.5148 - val_acc: 0.2205\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5068 - acc: 0.2625 - val_loss: 0.5119 - val_acc: 0.2695\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5047 - acc: 0.2955 - val_loss: 0.5104 - val_acc: 0.3000\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5035 - acc: 0.3150 - val_loss: 0.5095 - val_acc: 0.3335\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5029 - acc: 0.3575 - val_loss: 0.5090 - val_acc: 0.3120\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5024 - acc: 0.3815 - val_loss: 0.5091 - val_acc: 0.4270\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5022 - acc: 0.4045 - val_loss: 0.5084 - val_acc: 0.3920\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5019 - acc: 0.4275 - val_loss: 0.5084 - val_acc: 0.4330\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5018 - acc: 0.4315 - val_loss: 0.5082 - val_acc: 0.4190\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5016 - acc: 0.4580 - val_loss: 0.5081 - val_acc: 0.4630\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5014 - acc: 0.4610 - val_loss: 0.5080 - val_acc: 0.4595\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5014 - acc: 0.4645 - val_loss: 0.5075 - val_acc: 0.4550\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5013 - acc: 0.4750 - val_loss: 0.5076 - val_acc: 0.4765\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5012 - acc: 0.4875 - val_loss: 0.5075 - val_acc: 0.4790\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5011 - acc: 0.4855 - val_loss: 0.5077 - val_acc: 0.4870\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5011 - acc: 0.4855 - val_loss: 0.5074 - val_acc: 0.4880\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5010 - acc: 0.4925 - val_loss: 0.5073 - val_acc: 0.4855\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5010 - acc: 0.4920 - val_loss: 0.5075 - val_acc: 0.4915\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5010 - acc: 0.4945 - val_loss: 0.5072 - val_acc: 0.4885\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5009 - acc: 0.4950 - val_loss: 0.5070 - val_acc: 0.4905\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.5009 - acc: 0.4965 - val_loss: 0.5070 - val_acc: 0.4910\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5009 - acc: 0.4970 - val_loss: 0.5069 - val_acc: 0.4915\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5008 - acc: 0.4985 - val_loss: 0.5069 - val_acc: 0.4925\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5008 - acc: 0.4995 - val_loss: 0.5069 - val_acc: 0.4915\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5008 - acc: 0.4990 - val_loss: 0.5068 - val_acc: 0.4900\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5008 - acc: 0.4995 - val_loss: 0.5069 - val_acc: 0.4925\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5007 - acc: 0.4990 - val_loss: 0.5068 - val_acc: 0.4915\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5007 - acc: 0.4990 - val_loss: 0.5068 - val_acc: 0.4935\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5007 - acc: 0.4985 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5007 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5007 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5007 - acc: 0.4995 - val_loss: 0.5070 - val_acc: 0.4935\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5007 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5007 - acc: 0.4995 - val_loss: 0.5068 - val_acc: 0.4935\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5006 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5066 - val_acc: 0.4935\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5067 - val_acc: 0.4935\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5005 - acc: 0.4995 - val_loss: 0.5065 - val_acc: 0.4935\n",
      "2000/2000 [==============================] - 0s 10us/step\n",
      "2000/2000 [==============================] - 0s 12us/step\n",
      "Train loss: 0.501, Test loss: 0.507\n",
      "Train metric: 0.499, Test metric: 0.493\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=20, units=64, activation='relu'))\n",
    "model.add(Dense(units=1, activation='linear', kernel_regularizer=keras.regularizers.l2(l=0.01)))\n",
    "#compile the model\n",
    "model.compile(loss='hinge', optimizer=grid_result.best_params_['optimizer'], metrics=['accuracy'])\n",
    "#fit the model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=1)\n",
    "# evaluate the model\n",
    "train_e = model.evaluate(trainX, trainy, verbose=1)\n",
    "test_e = model.evaluate(testX, testy, verbose=1)\n",
    "print('Train loss: %.3f, Test loss: %.3f' % (train_e[0], test_e[0])) \n",
    "print('Train metric: %.3f, Test metric: %.3f' % (train_e[1], test_e[1])) \n",
    "#plot loss during training\n",
    "plt.title('Loss / Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://archive.md/sanjs\n",
    "https://archive.md/lOvhL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you program a simple tree with Keras Tensorflow? Keras layers do not include preprogrammed tree layers. Instead, Keras Tensforflow website includes tree and random forest models: https://archive.md/aic4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
